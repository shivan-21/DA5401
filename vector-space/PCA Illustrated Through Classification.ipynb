{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5a7qZi1Mj4YJ"
   },
   "source": [
    "#  Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCvy2Hmyj4YK"
   },
   "source": [
    "Up until now, we have been looking in depth at supervised learning estimators: those estimators that predict labels based on labeled training data.\n",
    "Here we begin looking at several unsupervised estimators, which can highlight interesting aspects of the data without reference to any known labels.\n",
    "\n",
    "In this chapter we will explore what is perhaps one of the most broadly used unsupervised algorithms, principal component analysis (PCA).\n",
    "PCA is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization, noise filtering, feature extraction and engineering, and much more.\n",
    "After a brief conceptual discussion of the PCA algorithm, we will explore a couple examples of these further applications.\n",
    "\n",
    "Original Source: https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0EbvsPbj4YL",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "import os\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(\"./\", fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivating Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    " \n",
    "X, y = make_moons(n_samples=500, noise=0.02, random_state=417)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=colors.ListedColormap([\"blue\", \"red\"]))\n",
    "plt.title(\"Moons Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__let's evaluate the baseline performance of classification on thie dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "yhat = model.predict(X)\n",
    "accuracy = int(100 * np.sum(y == yhat)/len(y))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=yhat, cmap=colors.ListedColormap([\"blue\", \"red\"]), alpha=1)\n",
    "\n",
    "# Refer: https://www.geeksforgeeks.org/ml-introduction-to-kernel-pca/\n",
    "# Retrieve the model parameters.\n",
    "b = model.intercept_[0]\n",
    "w1, w2 = model.coef_.T\n",
    "# Calculate the intercept and gradient of the decision boundary.\n",
    "c = -b/w2\n",
    "m = -w1/w2\n",
    "\n",
    "# Plot the data and the classification with the decision boundary.\n",
    "xmin, xmax = -1.25, 2.2\n",
    "ymin, ymax = -.6, 1.1\n",
    "xd = np.array([xmin, xmax])\n",
    "yd = m*xd + c\n",
    "plt.plot(xd, yd, 'k', lw=1, ls='--')\n",
    "plt.fill_between(xd, yd, ymin, color='tab:orange', alpha=0.2)\n",
    "plt.fill_between(xd, yd, ymax, color='tab:blue', alpha=0.2)\n",
    "\n",
    "plt.scatter(*X[y==0].T, s=8, alpha=0.5)\n",
    "plt.scatter(*X[y==1].T, s=8, alpha=0.5)\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.title(f\"Model Training Accuracy : {accuracy}%\")\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Given PCA is a linear model, we will see later how can solve the non-linearity in the data__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfpKJTa6j4YM"
   },
   "source": [
    "### Introducing Principal Component Analysis\n",
    "\n",
    "Principal component analysis is a fast and flexible unsupervised method for dimensionality reduction in data. Its behavior is easiest to visualize by looking at a two-dimensional dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's generate a 2d spherical data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note here that the mean is centered and the covariance matrix is the identity matrix\n",
    "ppx, ppy = np.random.multivariate_normal([0, 0], [[1, 0], [0, 1]], 5000).T\n",
    "plt.scatter(ppx, ppy, s=1, c='b')\n",
    "plt.gca().set_aspect(1)\n",
    "plt.autoscale()\n",
    "plt.title(\"2D Spherical Gaussian Distribution\")\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider these 1000 points sampled from a normal distribution that's a skewed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnRudn0yj4YM",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "7e8392ef-ee61-49a4-d206-29e57020c07f"
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X_1 = np.dot(rng.rand(2, 2), rng.randn(2, 1000)).T\n",
    "plt.scatter(X_1[:, 0], X_1[:, 1], s=3, c='blue')\n",
    "plt.title(\"Random 1000 points\")\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is apparent, that there is a nearly linear relationship between the *x* and *y* variables.\n",
    "This is reminiscent of the linear regression data we could explore in Linear Regression, but the problem setting here is slightly different: rather than attempting to *predict* the *y* values from the *x* values, the unsupervised learning problem attempts to learn about the *relationship* between the *$x_1$* and *$x_2$* values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSnYVPKuj4YM"
   },
   "source": [
    "\n",
    "\n",
    "In principal component analysis, this relationship is quantified by finding a list of the *principal axes* in the data, and using those axes to describe the dataset.\n",
    "Using Scikit-Learn's `PCA` estimator, we can compute this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgFQpbmSj4YN",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "8ef85002-7359-4993-84f0-15e51480a2a3"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ARmDufGj4YN"
   },
   "source": [
    "The fit learns some quantities from the data, most importantly the components and explained variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFO8tBU5j4YN",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "8cf171a1-97a6-4e7b-9a66-1838c41c761c"
   },
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuEFK5YAj4YN",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "67c15544-609f-49aa-f17f-ff16e272d3d8"
   },
   "outputs": [],
   "source": [
    "print(\"Variances:\", pca.explained_variance_)\n",
    "print(\"Variances (%):\", pca.explained_variance_ratio_)\n",
    "print(\"Total Variance (%) :\", 100*np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYyoUDArj4YN"
   },
   "source": [
    "To see what these numbers mean, let's visualize them as vectors over the input data, using the components to define the direction of the vector and the explained variance to define the squared length of the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-DCUt_qj4YO",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "1ee21e28-8cbb-4b10-d941-4eb917b4200e"
   },
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, name, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->', linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.text(*v1, s=name, fontsize=10, color='red')\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X_1[:, 0], X_1[:, 1], alpha=0.4, s=5)\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "\n",
    "for index, record in enumerate(zip(pca.explained_variance_, pca.components_)):\n",
    "    length, vector = record\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v, name=\"PC\"+str(index))\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdWhNlkUj4YO"
   },
   "source": [
    "These vectors represent the principal axes of the data, and the length of each vector is an indication of how \"important\" that axis is in describing the distribution of the data—more precisely, it is a measure of the variance of the data when projected onto that axis.\n",
    "The projection of each data point onto the principal axes are the principal components of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBjBgrBXj4YO"
   },
   "source": [
    "This transformation from data axes to principal axes is an *affine transformation*, which means it is composed of a translation, rotation, and uniform scaling.\n",
    "\n",
    "While this algorithm to find principal components may seem like just a mathematical curiosity, it turns out to have very far-reaching applications in the world of machine learning and data exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8zWcEsYj4YO"
   },
   "source": [
    "### PCA as Dimensionality Reduction\n",
    "\n",
    "Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.\n",
    "\n",
    "Here is an example of using PCA as a dimensionality reduction transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtPPpf4cj4YO",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "f8b187cc-642f-418c-80d3-e67dd893dea7"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "pca.fit(X_1)\n",
    "X_pca = pca.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ax5Yg8S6j4YO"
   },
   "source": [
    "The transformed data has been reduced to a single dimension.\n",
    "To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data (see the following figure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSyofliuj4YP",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "b9f0ebd8-0921-4396-c695-0410b16fe958"
   },
   "outputs": [],
   "source": [
    "X_new = pca.inverse_transform(X_pca)\n",
    "plt.scatter(X_1[:, 0], X_1[:, 1], alpha=0.2)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8, s=2, c='blue')\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BujJv9BTj4YP"
   },
   "source": [
    "The light points are the original data, while the dark points are the projected version.\n",
    "This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance.\n",
    "The fraction of variance that is cut out (proportional to the spread of points about the line formed in the preceding figure) is roughly a measure of how much \"information\" is discarded in this reduction of dimensionality.\n",
    "\n",
    "This reduced-dimension dataset is in some senses \"good enough\" to encode the most important relationships between the points: despite reducing the number of data features by 50%, the overall relationships between the data points are mostly preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's revisit the Moon's example by applying PCA.\n",
    "\n",
    "let's find out the base line performance of a classifier built on the original data and the PCA components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    " \n",
    "plt.title(\"PCA\")\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_pca, y)\n",
    "yhat = model.predict(X_pca)\n",
    "accuracy = int(100 * np.sum(y == yhat)/len(y))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=yhat, cmap=colors.ListedColormap([\"blue\", \"red\"]), alpha=1)\n",
    "\n",
    "# Retrieve the model parameters.\n",
    "b = model.intercept_[0]\n",
    "w1, w2 = model.coef_.T\n",
    "# Calculate the intercept and gradient of the decision boundary.\n",
    "c = -b/w2\n",
    "m = -w1/w2\n",
    "\n",
    "# Plot the data and the classification with the decision boundary.\n",
    "xmin, xmax = -1.6, 1.6\n",
    "ymin, ymax = -.8, .8\n",
    "xd = np.array([xmin, xmax])\n",
    "yd = m*xd + c\n",
    "plt.plot(xd, yd, 'k', lw=1, ls='--')\n",
    "plt.fill_between(xd, yd, ymin, color='tab:orange', alpha=0.2)\n",
    "plt.fill_between(xd, yd, ymax, color='tab:blue', alpha=0.2)\n",
    "\n",
    "plt.scatter(*X_pca[y==0].T, s=8, alpha=0.5)\n",
    "plt.scatter(*X_pca[y==1].T, s=8, alpha=0.5)\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.title(f\"Model Training Accuracy : {accuracy}%\")\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see is that the axis is rotated and the crescents appear baselined to the 45 degree line.  The performance didn't change from 88%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel PCA\n",
    "\n",
    "Let's try KernelPCA to capture the non-linearity in the data.  Let's also fit a LogReg classifier on the transformed data to compute the classification performance of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA(kernel='rbf', gamma=15, n_components=2)\n",
    "#kpca = KernelPCA(kernel='sigmoid', gamma=5, coef0=3.4, n_components=2)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    " \n",
    "model = LogisticRegression()\n",
    "model.fit(X_kpca, y)\n",
    "yhat = model.predict(X_kpca)\n",
    "accuracy = int(100 * np.sum(y == yhat)/len(y))\n",
    "plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=yhat, cmap=colors.ListedColormap([\"blue\", \"red\"]), alpha=1)\n",
    "\n",
    "plt.title(f\"Kernel PCA: Model Training Accuracy = {accuracy}%\")\n",
    "\n",
    "# Retrieve the model parameters.\n",
    "b = model.intercept_[0]\n",
    "w1, w2 = model.coef_.T\n",
    "# Calculate the intercept and gradient of the decision boundary.\n",
    "c = -b/w2\n",
    "m = -w1/w2\n",
    "\n",
    "# Plot the data and the classification with the decision boundary.\n",
    "xmin, xmax = -.4, .45\n",
    "ymin, ymax = -.6, .6\n",
    "\n",
    "# uncomment the following for the sigmoid kernel\n",
    "#xmin, xmax = -1., 1.5\n",
    "#ymin, ymax = -1., 1.\n",
    "xd = np.array([xmin, xmax])\n",
    "yd = m*xd + c\n",
    "plt.plot(xd, yd, 'k', lw=1, ls='--')\n",
    "plt.fill_between(xd, yd, ymin, color='tab:orange', alpha=0.2)\n",
    "plt.fill_between(xd, yd, ymax, color='tab:blue', alpha=0.2)\n",
    "\n",
    "plt.scatter(*X_kpca[y==0].T, s=8, alpha=0.5)\n",
    "plt.scatter(*X_kpca[y==1].T, s=8, alpha=0.5)\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.xlabel(r'$x_1$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Now we have achieved 100% training performance after the data got transformed into a linearly-separable one___\n",
    "\n",
    "\n",
    "Let's consider a slightly more complicated swiss roll dataset, which is an example of a non-linear manifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "\n",
    "axes = [-11.5, 14, -2, 23, -12, 15]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=t, cmap=plt.cm.winter)\n",
    "ax.view_init(10, -70)\n",
    "ax.set_xlabel(\"$x_1$\", fontsize=18)\n",
    "ax.set_ylabel(\"$x_2$\", fontsize=18)\n",
    "ax.set_zlabel(\"$x_3$\", fontsize=18)\n",
    "ax.set_xlim(axes[0:2])\n",
    "ax.set_ylim(axes[2:4])\n",
    "ax.set_zlim(axes[4:6])\n",
    "ax.set_title(\"Swiss Roll\")\n",
    "\n",
    "save_fig(\"swiss_roll_plot\")\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the 2-d views of the swiss roll dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11, 4))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=t, cmap=plt.cm.winter)\n",
    "#plt.axis(axes[:4])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\n",
    "plt.title('Top View')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.scatter(X[:, 0], X[:, 2], c=t, cmap=plt.cm.winter)\n",
    "#plt.axis([4, 15, axes[2], axes[3]])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$x_3$\", fontsize=18)\n",
    "plt.title('Front View')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.scatter(t, X[:, 1], c=t, cmap=plt.cm.winter)\n",
    "#plt.axis([4, 15, axes[2], axes[3]])\n",
    "plt.xlabel(\"Position\", fontsize=18)\n",
    "plt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\n",
    "plt.grid(True)\n",
    "\n",
    "save_fig(\"squished_swiss_roll_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct a classification dataset by thresholding the swiss roll.  We have the 't' variable that tracks the length of the swiss roll. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "lin_pca = KernelPCA(n_components = 2, kernel=\"linear\", fit_inverse_transform=True)\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\n",
    "sig_pca = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.001, coef0=1, fit_inverse_transform=True)\n",
    "\n",
    "# let's create a classification dataset!\n",
    "# by threshold on the position information (t)\n",
    "y = t > 7\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "for subplot, pca, title in ((131, lin_pca, \"Linear kernel\"), \n",
    "                            (132, rbf_pca, \"RBF kernel, $\\\\gamma=0.0433$\"), \n",
    "                            (133, sig_pca, \"Sigmoid kernel, $\\\\gamma=10^{-3}, r=1$\")):\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    if subplot == 132:\n",
    "        X_reduced_rbf = X_reduced\n",
    "    \n",
    "    plt.subplot(subplot)\n",
    "    plt.plot(X_reduced[y, 0], X_reduced[y, 1], \"g^\")\n",
    "    plt.plot(X_reduced[~y, 0], X_reduced[~y, 1], \"b.\")\n",
    "    plt.title(title, fontsize=14)\n",
    "    #plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.winter)\n",
    "    plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "    if subplot == 131:\n",
    "        plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "    plt.grid(True)\n",
    "\n",
    "save_fig(\"kernel_pca_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having visualized the data, let's try to learn the LogReg classifer on the transformed data to fit a linear classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty='l1', solver='saga')\n",
    "model.fit(X_reduced_rbf, y)\n",
    "yhat = model.predict(X_reduced_rbf)\n",
    "accuracy = int(100 * np.sum(y == yhat)/len(y))\n",
    "print(\"Model Training Accuracy :\", accuracy, \"%\")\n",
    "\n",
    "# Retrieve the model parameters.\n",
    "b = model.intercept_[0]\n",
    "w1, w2 = model.coef_.T\n",
    "# Calculate the intercept and gradient of the decision boundary.\n",
    "c = -b/w2\n",
    "m = -w1/w2\n",
    "\n",
    "# Plot the data and the classification with the decision boundary.\n",
    "xmin, xmax = -.25, .7\n",
    "ymin, ymax = -.35, .7\n",
    "xd = np.array([xmin, xmax])\n",
    "yd = m*xd + c\n",
    "plt.plot(xd, yd, 'k', lw=1, ls='--')\n",
    "plt.fill_between(xd, yd, ymin, color='tab:green', alpha=0.2)\n",
    "plt.fill_between(xd, yd, ymax, color='tab:blue', alpha=0.2)\n",
    "\n",
    "plt.scatter(*X_reduced_rbf[y==0].T, s=8, alpha=0.8)\n",
    "plt.scatter(*X_reduced_rbf[y==1].T, s=20, alpha=0.5, marker='^')\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.xlabel(r'$x_1$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqZcI75Ij4YP"
   },
   "source": [
    "### PCA for Visualization: Handwritten Digits\n",
    "\n",
    "The usefulness of dimensionality reduction may not be entirely apparent in only two dimensions, but it becomes clear when looking at high-dimensional data.\n",
    "To see this, let's take a quick look at the application of PCA to the digits dataset. We'll start by loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dnqYmLJVj4YP",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "aa715461-3268-4f40-c388-5ac1d7e346f4"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxg__6Bfj4YP"
   },
   "source": [
    "Recall that the digits dataset consists of 8 × 8–pixel images, meaning that they are 64-dimensional.\n",
    "To gain some intuition into the relationships between these points, we can use PCA to project them into a more manageable number of dimensions, say two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mOG0DAVxj4YP",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "ceb603ec-2d47-4d5b-d2ad-9ce8bba7178e"
   },
   "outputs": [],
   "source": [
    "pca = PCA(2)  # project from 64 to 2 dimensions\n",
    "projected = pca.fit_transform(digits.data)\n",
    "print(digits.data.shape)\n",
    "print(projected.shape)\n",
    "print(\"% of information preserved in components:\", pca.explained_variance_ratio_)\n",
    "print(\"total information in the reduced dim space :\", round(np.sum(pca.explained_variance_ratio_)* 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sp8zeA9zj4YP"
   },
   "source": [
    "We can now plot the first two principal components of each point to learn about the data, as seen in the following figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIIDPxj-j4YP",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "e9e15435-1792-479f-cd2b-11daa4460dba"
   },
   "outputs": [],
   "source": [
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=digits.target, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.get_cmap('rainbow', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chjMnGmsj4YQ"
   },
   "source": [
    "Recall what these components mean: the full data is a 64-dimensional point cloud, and these points are the projection of each data point along the directions with the largest variance.\n",
    "Essentially, we have found the optimal stretch and rotation in 64-dimensional space that allows us to see the layout of the data in two dimensions, and we have done this in an unsupervised manner—that is, without reference to the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V47Qx7M0j4YQ"
   },
   "source": [
    "### What Do the Components Mean?\n",
    "\n",
    "We can go a bit further here, and begin to ask what the reduced dimensions *mean*.\n",
    "This meaning can be understood in terms of combinations of basis vectors.\n",
    "For example, each image in the training set is defined by a collection of 64 pixel values, which we will call the vector $x$:\n",
    "\n",
    "$$\n",
    "x = [x_1, x_2, x_3 \\cdots x_{64}]\n",
    "$$\n",
    "\n",
    "One way we can think about this is in terms of a pixel basis.\n",
    "That is, to construct the image, we multiply each element of the vector by the pixel it describes, and then add the results together to build the image:\n",
    "\n",
    "$$\n",
    "{\\rm image}(x) = x_1 \\cdot{\\rm (pixel~1)} + x_2 \\cdot{\\rm (pixel~2)} + x_3 \\cdot{\\rm (pixel~3)} \\cdots x_{64} \\cdot{\\rm (pixel~64)}\n",
    "$$\n",
    "\n",
    "One way we might imagine reducing the dimensionality of this data is to zero out all but a few of these basis vectors.\n",
    "For example, if we use only the first eight pixels, we get an eight-dimensional projection of the data (the following figure). However, it is not very reflective of the whole image: we've thrown out nearly 90% of the pixels!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mil92Pkj4YQ"
   },
   "source": [
    "But the pixel-wise representation is not the only choice of basis. We can also use other basis functions, which each contain some predefined contribution from each pixel, and write something like:\n",
    "\n",
    "$$\n",
    "image(x) = {\\rm mean} + x_1 \\cdot{\\rm (basis~1)} + x_2 \\cdot{\\rm (basis~2)} + x_3 \\cdot{\\rm (basis~3)} \\cdots\n",
    "$$\n",
    "\n",
    "PCA can be thought of as a process of choosing optimal basis functions, such that adding together just the first few of them is enough to suitably reconstruct the bulk of the elements in the dataset.\n",
    "The principal components, which act as the low-dimensional representation of our data, are simply the coefficients that multiply each of the elements in this series.\n",
    "the following figure shows a similar depiction of reconstructing the same digit using the mean plus the first eight PCA basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1M2XmHgj4YQ"
   },
   "source": [
    "Unlike the pixel basis, the PCA basis allows us to recover the salient features of the input image with just a mean, plus eight components!\n",
    "The amount of each pixel in each component is the corollary of the orientation of the vector in our two-dimensional example.\n",
    "This is the sense in which PCA provides a low-dimensional representation of the data: it discovers a set of basis functions that are more efficient than the native pixel basis of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLqNC-L_j4YQ"
   },
   "source": [
    "### Choosing the Number of Components\n",
    "\n",
    "A vital part of using PCA in practice is the ability to estimate how many components are needed to describe the data.\n",
    "This can be determined by looking at the cumulative *explained variance ratio* as a function of the number of components (see the following figure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3j2UaMTj4YQ",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "4dd8b47a-1670-43c1-eb75-d88fd371e871"
   },
   "outputs": [],
   "source": [
    "pca = PCA().fit(digits.data)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmzcV4SOj4YQ"
   },
   "source": [
    "This curve quantifies how much of the total, 64-dimensional variance is contained within the first $N$ components.\n",
    "For example, we see that with the digits data the first 10 components contain approximately 75% of the variance, while you need around 50 components to describe close to 100% of the variance.\n",
    "\n",
    "This tells us that our 2-dimensional projection loses a lot of information (as measured by the explained variance) and that we'd need about 20 components to retain 90% of the variance.  Looking at this plot for a high-dimensional dataset can help you understand the level of redundancy present in its features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_39Fjknj4YR"
   },
   "source": [
    "## PCA as Noise Filtering\n",
    "\n",
    "PCA can also be used as a filtering approach for noisy data.\n",
    "The idea is this: any components with variance much larger than the effect of the noise should be relatively unaffected by the noise.\n",
    "So, if you reconstruct the data using just the largest subset of principal components, you should be preferentially keeping the signal and throwing out the noise.\n",
    "\n",
    "Let's see how this looks with the digits data.\n",
    "First we will plot several of the input noise-free input samples (the following figure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNQduynOj4YR",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "90c4e6cc-7dbf-40fe-98a0-0706fde8950b"
   },
   "outputs": [],
   "source": [
    "def plot_digits(data):\n",
    "    fig, axes = plt.subplots(4, 10, figsize=(10, 4),\n",
    "                             subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                             gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(data[i].reshape(8, 8),\n",
    "                  cmap='binary', interpolation='nearest',\n",
    "                  clim=(0, 16))\n",
    "plot_digits(digits.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyQeNSpAj4YR"
   },
   "source": [
    "Now let's add some random noise to create a noisy dataset, and replot it (the following figure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVRUJq4qj4YU",
    "outputId": "fb7bd05d-83d1-4fa6-eab3-876fe125d93b"
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "rng.normal(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enYQk-L6j4YU",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "2a962a9e-29b1-4ffb-b1dc-3345863cd1c2"
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "noisy = rng.normal(digits.data, 4)\n",
    "plot_digits(noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgPGH9JPj4YU"
   },
   "source": [
    "The visualization makes the presence of this random noise clear.\n",
    "Let's train a PCA model on the noisy data, requesting that the projection preserve 50% of the variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tzo_8Trwj4YU",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "caa6f226-a469-4e05-c686-552424cb46d8"
   },
   "outputs": [],
   "source": [
    "pca = PCA(0.50).fit(noisy)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5Fekxwrj4YU"
   },
   "source": [
    "Here 50% of the variance amounts to 12 principal components, out of the 64 original features.\n",
    "Now we compute these components, and then use the inverse of the transform to reconstruct the filtered digits; the following figure shows the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwUQGh37j4YU",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "8065dcb5-cbaf-486b-de5f-7c255e7aee43"
   },
   "outputs": [],
   "source": [
    "components = pca.transform(noisy)\n",
    "filtered = pca.inverse_transform(components)\n",
    "plot_digits(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHuDAlwVj4YU"
   },
   "source": [
    "This signal preserving/noise filtering property makes PCA a very useful feature selection routine—for example, rather than training a classifier on very high-dimensional data, you might instead train the classifier on the lower-dimensional principal component representation, which will automatically serve to filter out random noise in the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aRQoqPOj4YU"
   },
   "source": [
    "## Example: Eigenfaces\n",
    "\n",
    "Let's explore PCA projections of Labeled Faces in the Wild (LFW) dataset made available through Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szPCBbPkj4YU",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "f04eda1a-f635-47be-9775-d46220ea2461"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "print(faces.target_names)\n",
    "print(faces.images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhYwhA39j4YV"
   },
   "source": [
    "Let's take a look at the principal axes that span this dataset.\n",
    "Because this is a large dataset, we will use the `\"random\"` eigensolver in the `PCA` estimator: it uses a randomized method to approximate the first $N$ principal components more quickly than the standard approach, at the expense of some accuracy. This trade-off can be useful for high-dimensional data (here, a dimensionality of nearly 3,000).\n",
    "We will take a look at the first 150 components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-76C7dsbj4YV",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "028e5fbc-adcb-405c-c343-f8af93f40710"
   },
   "outputs": [],
   "source": [
    "pca = PCA(150, svd_solver='randomized', random_state=42)\n",
    "pca.fit(faces.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnRp1W1-j4YV"
   },
   "source": [
    "In this case, it can be interesting to visualize the images associated with the first several principal components (these components are technically known as *eigenvectors*,\n",
    "so these types of images are often called *eigenfaces*; as you can see in the following figure, they are as creepy as they sound):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rsKWPf2j4YV",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "70a51478-0687-43c7-e58f-483eba295525"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 8, figsize=(9, 4),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(pca.components_[i].reshape(62, 47), cmap='bone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjWnt0AMj4YV"
   },
   "source": [
    "The results are very interesting, and give us insight into how the images vary: for example, the first few eigenfaces (from the top left) seem to be associated with the angle of lighting on the face, and later principal vectors seem to be picking out certain features, such as eyes, noses, and lips.\n",
    "Let's take a look at the cumulative variance of these components to see how much of the data information the projection is preserving (see the following figure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWWHmjHxj4YV",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "fd4dd2b7-42a4-4e87-ceb4-c650f53ad2fe"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ESkUWuWj4YV"
   },
   "source": [
    "The 150 components we have chosen account for just over 90% of the variance.\n",
    "That would lead us to believe that using these 150 components, we would recover most of the essential characteristics of the data.\n",
    "To make this more concrete, we can compare the input images with the images reconstructed from these 150 components (see the following figure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orWbEWD2j4YV",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute the components and projected faces\n",
    "pca = pca.fit(faces.data)\n",
    "components = pca.transform(faces.data)\n",
    "projected = pca.inverse_transform(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zw5odld3j4YV",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "1af7279b-8a8c-4329-d054-19a94ab6032a"
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(2, 10, figsize=(10, 2.5),\n",
    "                       subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                       gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap='binary_r')\n",
    "    ax[1, i].imshow(projected[i].reshape(62, 47), cmap='binary_r')\n",
    "\n",
    "ax[0, 0].set_ylabel('full-dim\\ninput')\n",
    "ax[1, 0].set_ylabel('150-dim\\nreconstruction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2U84XyFj4YV"
   },
   "source": [
    "The top row here shows the input images, while the bottom row shows the reconstruction of the images from just 150 of the ~3,000 initial features.\n",
    "This visualization makes clear why the PCA feature selection is successful: although it reduces the dimensionality of the data by nearly a factor of 20, the projected images contain enough information that we might, by eye, recognize the individuals in each image. This means our classification algorithm only needs to be trained on 150-dimensional data rather than 3,000-dimensional data, which, depending on the particular algorithm we choose, can lead to much more efficient classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for Compression\n",
    "\n",
    "Let's consider the MNIST 784 dataset where each image is 28x28 gray scale.  We will try to compress the images into lesser principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "mnist.target = mnist.target.astype(np.uint8)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=154)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "def plot_digits(instances, images_per_row=5, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    # This is equivalent to n_rows = ceil(len(instances) / images_per_row):\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "\n",
    "    # Append empty images to fill the end of the grid, if needed:\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    padded_instances = np.concatenate([instances, np.zeros((n_empty, size * size))], axis=0)\n",
    "\n",
    "    # Reshape the array so it's organized as a grid containing 28×28 images:\n",
    "    image_grid = padded_instances.reshape((n_rows, images_per_row, size, size))\n",
    "\n",
    "    # Combine axes 0 and 2 (vertical image grid axis, and vertical image axis),\n",
    "    # and axes 1 and 3 (horizontal axes). We first need to move the axes that we\n",
    "    # want to combine next to each other, using transpose(), and only then we\n",
    "    # can reshape:\n",
    "    big_image = image_grid.transpose(0, 2, 1, 3).reshape(n_rows * size,\n",
    "                                                         images_per_row * size)\n",
    "    # Now that we have a big image, we just need to show it:\n",
    "    plt.imshow(big_image, cmap = mpl.cm.binary, **options)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "plt.subplot(121)\n",
    "plot_digits(X_train[::2100])\n",
    "plt.title(\"Original\", fontsize=16)\n",
    "plt.subplot(122)\n",
    "plot_digits(X_recovered[::2100])\n",
    "plt.title(\"Compressed\", fontsize=16)\n",
    "\n",
    "save_fig(\"mnist_compression_plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental PCA\n",
    "\n",
    "Incremental principal component analysis (IPCA) is typically used as a replacement for principal component analysis (PCA) when the dataset to be decomposed is too large to fit in memory. IPCA builds a low-rank approximation for the input data using an amount of memory which is independent of the number of input data samples. It is still dependent on the input data features, but changing the batch size allows for control of memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    print(\".\", end=\"\") \n",
    "    inc_pca.partial_fit(X_batch)\n",
    "\n",
    "X_ipca = inc_pca.transform(X_train)\n",
    "X_train_ipca = inc_pca.inverse_transform(X_ipca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the recostructed image from IPCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "plt.subplot(121)\n",
    "plot_digits(X_train[::3000])\n",
    "plt.title(\"Original\", fontsize=16)\n",
    "plt.subplot(122)\n",
    "plot_digits(X_train_ipca[::3000])\n",
    "plt.title(\"Compressed\", fontsize=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized PCA\n",
    "\n",
    "Run randomized SVD by the method of Halko et al.  Linear dimensionality reduction using approximated Singular Value Decomposition of the data and keeping only the most significant singular vectors to project the data to a lower dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\", random_state=42)\n",
    "\n",
    "X_rpca = rnd_pca.fit_transform(X_train)\n",
    "X_train_rpca = inc_pca.inverse_transform(X_rpca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the output from RPCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "plt.subplot(121)\n",
    "plot_digits(X_train[::1900])\n",
    "plt.title(\"Original\", fontsize=16)\n",
    "plt.subplot(122)\n",
    "plot_digits(X_train_rpca[::1900])\n",
    "plt.title(\"Compressed\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side-by-side comparison now!\n",
    "\n",
    "Let's compare the outputs from each of the techniques.  It will be obvious that the randomized PCA's recovery was of the lowest quality.   And we observe that the quality of PCA and Incremental PCA is indistinguishable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(\"./\", fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(131)\n",
    "plot_digits(X_recovered[::2100])\n",
    "plt.title(\"Compressed (PCA)\", fontsize=16)\n",
    "plt.subplot(132)\n",
    "plot_digits(X_train_ipca[::2100])\n",
    "plt.title(\"Compressed (Inc PCA)\", fontsize=16)\n",
    "plt.subplot(133)\n",
    "plot_digits(X_train_rpca[::2100])\n",
    "plt.title(\"Compressed (Rnd PCA)\", fontsize=16)\n",
    "\n",
    "\n",
    "save_fig(\"mnist_compression_plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the Time Complexity\n",
    "\n",
    "Let's time regular PCA against Incremental PCA and Randomized PCA, for various number of principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for n_components in (2, 10, 154):\n",
    "    print(\"n_components =\", n_components)\n",
    "    regular_pca = PCA(n_components=n_components, svd_solver=\"full\")\n",
    "    inc_pca = IncrementalPCA(n_components=n_components, batch_size=500)\n",
    "    rnd_pca = PCA(n_components=n_components, random_state=42, svd_solver=\"randomized\")\n",
    "\n",
    "    for name, pca in ((\"PCA\", regular_pca), (\"Inc PCA\", inc_pca), (\"Rnd PCA\", rnd_pca)):\n",
    "        t1 = time.time()\n",
    "        pca.fit(X_train)\n",
    "        t2 = time.time()\n",
    "        print(\"    {}: {:.1f} seconds\".format(name, t2 - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFgiM8Qyj4YW"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this chapter we explored the use of principal component analysis for dimensionality reduction, visualization of high-dimensional data, noise filtering, and feature selection within high-dimensional data.\n",
    "Because of its versatility and interpretability, PCA has been shown to be effective in a wide variety of contexts and disciplines.\n",
    "Given any high-dimensional dataset, I tend to start with PCA in order to visualize the relationships between points (as we did with the digits data), to understand the main variance in the data (as we did with the eigenfaces), and to understand the intrinsic dimensionality (by plotting the explained variance ratio).\n",
    "Certainly PCA is not useful for every high-dimensional dataset, but it offers a straightforward and efficient path to gaining insight into high-dimensional data.\n",
    "\n",
    "PCA's main weakness is that it tends to be highly affected by outliers in the data.\n",
    "For this reason, several robust variants of PCA have been developed, many of which act to iteratively discard data points that are poorly described by the initial components.\n",
    "Scikit-Learn includes a number of interesting variants on PCA in the `sklearn.decomposition` submodule; one example is `SparsePCA`, which introduces a regularization term that serves to enforce sparsity of the components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Image Compression Demo\n",
    "\n",
    "https://timbaumann.info/svd-image-compression-demo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "da5401",
   "language": "python",
   "name": "da5401"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
